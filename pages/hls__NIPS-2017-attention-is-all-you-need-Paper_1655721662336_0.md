file-path:: ../assets/NIPS-2017-attention-is-all-you-need-Paper_1655721662336_0.pdf
file:: [NIPS-2017-attention-is-all-you-need-Paper_1655721662336_0.pdf](../assets/NIPS-2017-attention-is-all-you-need-Paper_1655721662336_0.pdf)
title:: hls__NIPS-2017-attention-is-all-you-need-Paper_1655721662336_0

- The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder. 
  ls-type:: annotation
  hl-page:: 1
  id:: 62b04fd4-c1bc-4163-b9cd-5436e45d8cda
- attentionmechanism
  ls-type:: annotation
  hl-page:: 1
  id:: 62b05042-a07c-419a-a40f-32abc45269f8
- simple network architecture
  ls-type:: annotation
  hl-page:: 1
  id:: 62b0507c-2d24-47fe-8bdf-a89d9f8b3229
- multi-headed self-attention
  ls-type:: annotation
  hl-page:: 9
  id:: 62b05374-42cb-43f5-885c-24f8925fadbf
- auto-regressive
  ls-type:: annotation
  hl-page:: 2
  id:: 62b1ab30-7555-40ed-badf-64282aec2c28
- [:span]
  ls-type:: annotation
  hl-page:: 3
  id:: 62b1ad52-c75c-42fe-b65a-39963370b138
  hl-type:: area
  hl-stamp:: 1655811409198