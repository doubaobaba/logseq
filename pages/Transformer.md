- ![NIPS-2017-attention-is-all-you-need-Paper.pdf](../assets/NIPS-2017-attention-is-all-you-need-Paper_1655721662336_0.pdf)
- 摘要
	- 在主流的[[序列转录模型]]里面，这样的模型主要是以来比较复杂的循环 ((62b04fd4-c1bc-4163-b9cd-5436e45d8cda))
	- 在性能最好的模型里，通常也会在编码器和解码器之间，使用一个叫注意力机制 ((62b05042-a07c-419a-a40f-32abc45269f8)) 的东西
	- 这篇文章提出一个新的简单网络架构 ((62b0507c-2d24-47fe-8bdf-a89d9f8b3229)) （不是 novel，而是 simple）Transformer（变形金刚），模型仅仅依赖于注意力机制，而没有用之前的循环或者是卷积
	- 作者做了两个机器翻译的实验，显示模型性能上特别好。并行度更好，可以用更少的时间来训练。
	- 模型达到了 28.4 [[BLEU]]，做英语到德语的翻译工作，比目前最好的结果好了两个 BLEU
		- 在英语到法语的翻译，做了一个单模型，比所有的模型效果都要好，只在 8 个 GPU 上训练 3.5 天
	- 最后说 Transformer 架构能够泛化到一些别的任务上，效果也很好
- 结论
	- 介绍了 Transformer 模型，是第一个做序列转录的模型，仅仅使用了注意力把职权所有的循环层，全部换成了 ((62b05374-42cb-43f5-885c-24f8925fadbf))
	- 在机器翻译这个任务上面，Transformer 能够训练得比其他架构都要快很多，而且在实际的结果上确实是效果比较好
	- 对基于纯注意力的模型感到非常激动，想把它用在一些别的除文本以外的任务上面，包括图片、语音和视频，使得生成不那么时序化
- 导言
	- 在时序模型里，当前（2017 年，现在是 RNN）最常用的是 LSTM、GRU
	- 两个主流模型：语言模型、输出结构化信息比较多时，会用一个叫编码器和解码器的架构