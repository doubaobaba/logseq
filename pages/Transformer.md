- ![NIPS-2017-attention-is-all-you-need-Paper.pdf](../assets/NIPS-2017-attention-is-all-you-need-Paper_1655721662336_0.pdf)
- # 摘要
	- 在主流的[[序列转录模型]]里面，这样的模型主要是以来比较复杂的循环 ((62b04fd4-c1bc-4163-b9cd-5436e45d8cda))
	- 在性能最好的模型里，通常也会在编码器和解码器之间，使用一个叫注意力机制 ((62b05042-a07c-419a-a40f-32abc45269f8)) 的东西
	- 这篇文章提出一个新的简单网络架构 ((62b0507c-2d24-47fe-8bdf-a89d9f8b3229)) （不是 novel，而是 simple）Transformer（变形金刚），模型仅仅依赖于注意力机制，而没有用之前的循环或者是卷积
	- 作者做了两个机器翻译的实验，显示模型性能上特别好。并行度更好，可以用更少的时间来训练。
	- 模型达到了 28.4 [[BLEU]]，做英语到德语的翻译工作，比目前最好的结果好了两个 BLEU
		- 在英语到法语的翻译，做了一个单模型，比所有的模型效果都要好，只在 8 个 GPU 上训练 3.5 天
	- 最后说 Transformer 架构能够泛化到一些别的任务上，效果也很好
- # 结论
	- 介绍了 Transformer 模型，是第一个做序列转录的模型，仅仅使用了注意力把职权所有的循环层，全部换成了 ((62b05374-42cb-43f5-885c-24f8925fadbf))
	- 在机器翻译这个任务上面，Transformer 能够训练得比其他架构都要快很多，而且在实际的结果上确实是效果比较好
	- 对基于纯注意力的模型感到非常激动，想把它用在一些别的除文本以外的任务上面，包括图片、语音和视频，使得生成不那么时序化
- # 导言
	- 在时序模型里，当前（2017 年，现在是 RNN）最常用的是 LSTM、GRU
	- 两个主流模型：语言模型、输出结构化信息比较多时，会用一个叫编码器和解码器的架构
	- RNN 的特点：给一个序列，它的计算是把这个序列从左往右移一步一步往前做。比如是个句子，就是一个词一个词地看，对第 t 个值，会计算一个输出叫做 ht，ht 是由前面一个词的隐藏状态(ht-1)和当前第 t 个词本身决定的，这样就能把前面学到的历史信息，通过 ht-1 放到当下，和当前的词做计算，得到输出。这是 RNN 能够有效处理时序信息的一个关键之所在，即把之前的信息放在隐藏状态，然后一个一个放下去
	- RNN 的缺点：是一个时序，一步步运算，难以并行。即算 ht 时，必须要保证前面的 ht-1 输入完成了
		- 历史信息是一步一步往后传递，如果时序比较长，在早期的时序信息在后面可能会丢掉。如果不想丢失，就得做一个大的 ht，在每一个时间都得存下来，导致内存开销太大
	- attention 在 RNN 上的应用
	- 提出 Transformer 新模型，纯基于注意力机制，并行度高，能在短的时间里达到更好的效果
- # 相关工作(Background)
	- 如何使用卷积神经网络来替换循环神经网络，减少时序的计算
		- 用卷积神经网络，对长的序列难以建模。因为卷积做计算时，每次去看一个比较小的窗口，比如 3 * 3 的像素块，如果两个像素相隔比较远，得需要用很多层卷积，一层一层卷积，才能够最后把两个隔得远的像素融合起来
		- 如果使用 Transformer 里的注意力机制，每一次能看到所有的像素，所以一层就能把整个序列看到
		- 卷积比较好的地方是可以做多个输出通道，一个输出通道可以认为是可以去识别不一样的模式
		- Multi-Head Attention 多头注意力机制
			- 可以模拟卷积神经网络多输出通道的效果
	- 自注意力机制，之前已经有人提过了
	- memory networks
	- To the best of our knowledge, Transformer 是第一个只依赖于自注意力来做 encode 和 decode 架构的模型
	- 关键要讲清楚与论文相关的论文、联系是什么、区别是什么
- # 模型架构
	- 序列模型中，比较好的是编码器和解码器的架构
	- 编码器
		- 将一个长为 $n$ 的 $x_1$ 一直到 $x_n$ 的输入，假设一个句子有 $n$ 个词，$x_t$ 就表示第 $t$ 个词。编码器会把这个序列表示为也是长为 $n$，但其中每个 $z_t$ 对应的是 $x_t$ 的向量的表示。同样假设有一个句子，$z_t$ 表示第 $t$ 个词的向量表示，$z$ 也就是编码器的输出
	- 解码器
		- 会拿到编码器的输出，生成一个长为 $m$ 的一个序列（$m$ 和 $n$ 不一定等长，比如英译汉）
		- 和编码器最大的区别：解码器中词是一个一个生成的，而编码器可以一次性看全整个句子。
		- 比如做翻译时可以整个句子返回，解码只能一个一个生成，这叫做自回归 ((62b1ab30-7555-40ed-badf-64282aec2c28))，这里面输入又是输出
			- 最开始给定 $z$，去生成第一个输出 $y_1$，拿到 $y_1$ 后就可以生成 $y_2$。一般来说要生成 $y_t$ 的话，把之前所有的 $y_1$ 到 $y_t - 1$ 拿到，即过去时刻的输出也是当前时刻的输入
	- Transformer 使用了编码器和解码器的架构，具体是将一些自注意力和 point-wise fully connected layers 一个一个堆在一起
	  ((62b1ad52-c75c-42fe-b65a-39963370b138)) （写论文的话，有一张漂亮的、能把整个全局画清楚的图非常重要！）
		- （这张图在没有看到后面内容之前，是看不太懂在说什么的）
		- 左半边是编码器，右半边是解码器
	-
	-