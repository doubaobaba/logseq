{:highlights [{:id #uuid "62b04fd4-c1bc-4163-b9cd-5436e45d8cda", :page 1, :position {:bounding {:x1 249.359375, :y1 620.640625, :x2 811.724365234375, :y2 662.546875, :width 1060.8, :height 1372.8}, :rects ({:x1 249.359375, :y1 620.640625, :x2 811.724365234375, :y2 643.640625, :width 1060.8, :height 1372.8} {:x1 249.359375, :y1 639.546875, :x2 746.7132568359375, :y2 662.546875, :width 1060.8, :height 1372.8}), :page 1}, :content {:text "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder. "}, :properties {:color "blue"}} {:id #uuid "62b05042-a07c-419a-a40f-32abc45269f8", :page 1, :position {:bounding {:x1 249.359375, :y1 658.453125, :x2 811.4282836914062, :y2 700.375, :width 1060.8, :height 1372.8}, :rects ({:x1 747.2488403320312, :y1 658.453125, :x2 811.4282836914062, :y2 681.453125, :width 1060.8, :height 1372.8} {:x1 249.359375, :y1 677.375, :x2 332.3797607421875, :y2 700.375, :width 1060.8, :height 1372.8}), :page 1}, :content {:text "attentionmechanism"}, :properties {:color "yellow"}} {:id #uuid "62b0507c-2d24-47fe-8bdf-a89d9f8b3229", :page 1, :position {:bounding {:x1 485.2265625, :y1 677.375, :x2 687.5164794921875, :y2 700.375, :width 1060.8, :height 1372.8}, :rects ({:x1 485.2265625, :y1 677.375, :x2 687.5164794921875, :y2 700.375, :width 1060.8, :height 1372.8}), :page 1}, :content {:text "simple network architecture"}, :properties {:color "yellow"}} {:id #uuid "62b05374-42cb-43f5-885c-24f8925fadbf", :page 9, :position {:bounding {:x1 187.1875, :y1 948, :x2 374.6524658203125, :y2 971, :width 1060.8, :height 1372.8}, :rects ({:x1 187.1875, :y1 948, :x2 374.6524658203125, :y2 971, :width 1060.8, :height 1372.8}), :page 9}, :content {:text "multi-headed self-attention"}, :properties {:color "yellow"}}]}